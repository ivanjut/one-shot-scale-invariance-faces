{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_path = '../9520-final-project/SUFRData/image_files/uniform_bg/scaling'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg19 = models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /Users/ivanjutamulia/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n",
      "100%|██████████| 170M/170M [00:08<00:00, 21.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "resnet101 = models.resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: use more Brain-Score models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove last layer from network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19.classifier = nn.Sequential(*list(vgg19.classifier.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet101 = nn.Sequential(*list(resnet101.children())[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_img(image_path, img_idx):\n",
    "    return Image.open('{}/{}.jpg'.format(image_path, img_idx))\n",
    "\n",
    "def process_img(img):    \n",
    "    transform = transforms.Compose([\n",
    "     transforms.Resize(256),                    \n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "         mean=[0.485, 0.456, 0.406],\n",
    "         std=[0.229, 0.224, 0.225]                  \n",
    "    )])\n",
    "    img = transform(img)\n",
    "    processed_img = torch.unsqueeze(img, 0)\n",
    "    \n",
    "    return processed_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: make datasets with specific scale differences to see if there is difference with more scale variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(relative_path):\n",
    "    f = open(relative_path, \"r\")\n",
    "    dataset_idxs = []\n",
    "    for line in f.readlines():\n",
    "        img1_idx, img2_idx, label = line.strip().split(\" \")\n",
    "        dataset_idxs.append(((img1_idx, img2_idx), label))\n",
    "    return dataset_idxs\n",
    "\n",
    "def get_negative_pairs(face_idxs, c_offset_values):\n",
    "    negative_pairs = []\n",
    "    for c in c_offset_values:\n",
    "        for i in face_idxs:\n",
    "            for j in face_idxs:\n",
    "                # ensures i != j and that there are no duplicate pairs\n",
    "                if i > j: \n",
    "                    pair_idxs = (1+5*i+2,1+5*j+2+c)\n",
    "                    negative_pairs.append(pair_idxs)\n",
    "    return negative_pairs\n",
    "\n",
    "def get_dataset_idxs(face_idxs, c_offset_values, num_negative_pairs=None):\n",
    "    if num_negative_pairs is None:\n",
    "        num_negative_pairs = face_idxs.shape[0] * len(c_offset_values)\n",
    "    positive_pairs = np.array([(1+5*i+2,1+5*i+2+c) for c in c_offset_values for i in face_idxs])\n",
    "    all_negative_pairs = np.array(get_negative_pairs(face_idxs, c_offset_values))\n",
    "    negative_pairs = all_negative_pairs[list(np.random.choice(len(all_negative_pairs), num_negative_pairs, replace=False)), :]\n",
    "    return positive_pairs, negative_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_face_idxs = np.random.choice(400, 350, replace=False)\n",
    "\n",
    "mask = np.ones(400, dtype=bool)\n",
    "mask[train_face_idxs] = False\n",
    "test_face_idxs = np.arange(400)[mask]\n",
    "c_offset_values = [-2,-1,1,2]\n",
    "\n",
    "train_positive_pairs, train_negative_pairs = get_dataset_idxs(train_face_idxs, c_offset_values)\n",
    "test_positive_pairs, test_negative_pairs = get_dataset_idxs(test_face_idxs, [-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a,b):\n",
    "    dot = np.dot(a, b.T)\n",
    "    norm_product = np.linalg.norm(a)*np.linalg.norm(b)\n",
    "    return dot / norm_product\n",
    "\n",
    "def compare_img_pairs(model, train_idxs, use_corr=False):\n",
    "    similarities = []\n",
    "    i = 0\n",
    "    \n",
    "    for img1_idx, img2_idx in train_idxs:\n",
    "        img1 = process_img(load_img(image_path, img1_idx))\n",
    "        img2 = process_img(load_img(image_path, img1_idx))\n",
    "        vector1 = model(img1).detach().numpy()\n",
    "        vector2 = model(img2).detach().numpy()\n",
    "        \n",
    "        if use_corr:\n",
    "            similarities.append(np.corrcoef(vector1, vector2)[0][1])\n",
    "        else:\n",
    "            similarities.append(cos_sim(vector1, vector2)[0][0])\n",
    "#         print(i)\n",
    "        i += 1\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_pair_similarities = compare_img_pairs(vgg19, test_positive_pairs, use_corr=False)\n",
    "negative_pair_similarities = compare_img_pairs(vgg19, test_negative_pairs, use_corr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4096328"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(positive_pair_similarities).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41143674"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(negative_pair_similarities).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precompute and Store Img. Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images.\n",
      "Processed 200 images.\n",
      "Processed 300 images.\n",
      "Processed 400 images.\n",
      "Processed 500 images.\n",
      "Processed 600 images.\n",
      "Processed 700 images.\n",
      "Processed 800 images.\n",
      "Processed 900 images.\n",
      "Processed 1000 images.\n",
      "Processed 1100 images.\n",
      "Processed 1200 images.\n",
      "Processed 1300 images.\n",
      "Processed 1400 images.\n",
      "Processed 1500 images.\n",
      "Processed 1600 images.\n",
      "Processed 1700 images.\n",
      "Processed 1800 images.\n",
      "Processed 1900 images.\n",
      "Processed 2000 images.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "vgg19_img_idx_to_vec = {}\n",
    "\n",
    "for img_idx in range(1, 2001):\n",
    "    img_vec = process_img(load_img(image_path, img_idx))\n",
    "    img_feature_vec = vgg19.forward(img_vec).detach().squeeze().numpy()\n",
    "    vgg19_img_idx_to_vec[img_idx] = img_feature_vec.tolist()\n",
    "    if img_idx % 100 == 0:\n",
    "        print(\"Processed {} images.\".format(img_idx))\n",
    "\n",
    "with open('./vgg19-feature-vecs.json', 'w') as f:\n",
    "    json.dump(vgg19_img_idx_to_vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_img_feature_vecs(model, modelname):\n",
    "    model_img_idx_to_vec = {}\n",
    "\n",
    "    for img_idx in range(1, 2001):\n",
    "        img_vec = process_img(load_img(image_path, img_idx))\n",
    "        img_feature_vec = model.forward(img_vec).detach().squeeze().numpy()\n",
    "        model_img_idx_to_vec[img_idx] = img_feature_vec.tolist()\n",
    "        if img_idx % 100 == 0:\n",
    "            print(\"Processed {} images.\".format(img_idx))\n",
    "\n",
    "    with open('./{}-feature-vecs.json'.format(modelname), 'w') as f:\n",
    "        json.dump(model_img_idx_to_vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 images.\n",
      "Processed 200 images.\n",
      "Processed 300 images.\n",
      "Processed 400 images.\n",
      "Processed 500 images.\n",
      "Processed 600 images.\n",
      "Processed 700 images.\n",
      "Processed 800 images.\n",
      "Processed 900 images.\n",
      "Processed 1000 images.\n",
      "Processed 1100 images.\n",
      "Processed 1200 images.\n",
      "Processed 1300 images.\n",
      "Processed 1400 images.\n",
      "Processed 1500 images.\n",
      "Processed 1600 images.\n",
      "Processed 1700 images.\n",
      "Processed 1800 images.\n",
      "Processed 1900 images.\n",
      "Processed 2000 images.\n"
     ]
    }
   ],
   "source": [
    "store_img_feature_vecs(resnet101, 'resnet101')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 0: Similarity-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC Score (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49179999999999996"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "labels = np.concatenate((np.zeros(len(negative_pair_similarities)), np.ones(len(positive_pair_similarities))))\n",
    "roc_auc_score(labels, negative_pair_similarities + positive_pair_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1: MLP Same/Different Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_img_idx_to_vec = {}\n",
    "\n",
    "def get_vgg_vec(img_idx, vgg_img_idx_to_vec, model, image_path):\n",
    "    if img_idx in vgg_img_idx_to_vec:\n",
    "        return vgg_img_idx_to_vec[img_idx]\n",
    "    else:\n",
    "        img_vec = process_img(load_img(image_path, img_idx))\n",
    "        img_feature_vec = model.forward(img_vec)\n",
    "        vgg_img_idx_to_vec[img_idx] = img_feature_vec\n",
    "        return img_feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_preprocessing(model, positive_pairs, negative_pairs):\n",
    "\n",
    "    pairs = np.concatenate((positive_pairs, negative_pairs))\n",
    "\n",
    "    x = []\n",
    "    for img1_idx, img2_idx in pairs:\n",
    "        img1_feature_vec = get_vgg_vec(img1_idx, vgg_img_idx_to_vec, model, image_path)\n",
    "        img2_feature_vec = get_vgg_vec(img2_idx, vgg_img_idx_to_vec, model, image_path)\n",
    "        x_i = torch.cat([img1_feature_vec, img2_feature_vec], dim=1).detach().squeeze().numpy()\n",
    "        x.append(x_i)\n",
    "\n",
    "    x = np.array(x)\n",
    "    y = np.concatenate((np.zeros(len(negative_pairs)), np.ones(len(positive_pairs))))\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 8192)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=[512,64,10])\n",
    "train_x, train_y = mlp_preprocessing(vgg19, train_positive_pairs[:10], train_negative_pairs[:10])\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020002258602114507"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlp.fit(train_x,train_y)\n",
    "model.best_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x, test_y = mlp_preprocessing(vgg19, test_positive_pairs, test_negative_pairs)\n",
    "model.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation 2: MLP Multiclass Classification and Correlation of feature vector for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750\n",
      "1750\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "train_faces = list(chain.from_iterable(train_positive_pairs)) + list(chain.from_iterable(train_negative_pairs))\n",
    "train_faces_indices = np.unique(train_faces)\n",
    "train_faces_labels = [(i-1) // 5 for i in train_faces_indices]\n",
    "print(len(train_faces_indices))\n",
    "print(len(train_faces_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2_preprocessing(model, train_faces_indices):\n",
    "    x = []\n",
    "    for i in train_faces_indices:\n",
    "        img_feature_vec = get_vgg_vec(i, vgg_img_idx_to_vec, model, image_path).detach().squeeze().numpy()\n",
    "        x.append(img_feature_vec)\n",
    "\n",
    "    x = np.array(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=[512,64,10])\n",
    "\n",
    "train_faces = mlp2_preprocessing(vgg19, train_faces_indices[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010844428949991225"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = mlp2.fit(train_faces, train_faces_labels[:50])\n",
    "model2.best_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2_preprocessing_test_pairs(model, test_faces_indices, positive=True):\n",
    "    x = []\n",
    "    if positive:\n",
    "        for i in test_faces_indices:\n",
    "            img_feature_vec = get_vgg_vec(i, vgg_img_idx_to_vec, model, image_path).detach().squeeze().numpy()\n",
    "            x.append(img_feature_vec)\n",
    "\n",
    "        x = np.array(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_faces_positive_pair = mlp2_preprocessing(vgg19, train_faces_indices[53:55])\n",
    "test_faces_negative_pair = mlp2_preprocessing(vgg19, train_faces_indices[63:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network._base import ACTIVATIONS\n",
    "\n",
    "def deepest_layer(data, MLP, layer=0):\n",
    "    L = ACTIVATIONS['relu'](np.matmul(data, MLP.coefs_[layer]) + MLP.intercepts_[layer])\n",
    "    layer += 1\n",
    "    if layer >= len(MLP.coefs_)-1:\n",
    "        return L\n",
    "    else:\n",
    "        return deepest_layer(L, MLP, layer=layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_pos = deepest_layer(test_faces_positive_pair, model2)\n",
    "L_neg = deepest_layer(test_faces_negative_pair, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9871656403115372"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(L_pos[0], L_pos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9351490858375113"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim(L_neg[0], L_neg[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
